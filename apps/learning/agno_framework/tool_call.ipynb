{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f34038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> OPENAI_API_KEY not set. Please set the OPENAI_API_KEY environment variable.                               \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mERROR   \u001b[0m OPENAI_API_KEY not set. Please set the OPENAI_API_KEY environment variable.                               \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> Error from OpenAI API: The api_key client option must be set either by passing api_key to the client or by\n",
       "         setting the OPENAI_API_KEY environment variable                                                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mERROR   \u001b[0m Error from OpenAI API: The api_key client option must be set either by passing api_key to the client or by\n",
       "         setting the OPENAI_API_KEY environment variable                                                           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModelProviderError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Learning/Galaxy-Of-AI-API-Hub/venv/lib/python3.12/site-packages/agno/models/openai/chat.py:441\u001b[39m, in \u001b[36mOpenAIChat.invoke_stream\u001b[39m\u001b[34m(self, messages)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.chat.completions.create(\n\u001b[32m    442\u001b[39m         model=\u001b[38;5;28mself\u001b[39m.id,\n\u001b[32m    443\u001b[39m         messages=[\u001b[38;5;28mself\u001b[39m._format_message(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages],  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    444\u001b[39m         stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    445\u001b[39m         stream_options={\u001b[33m\"\u001b[39m\u001b[33minclude_usage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[32m    446\u001b[39m         **\u001b[38;5;28mself\u001b[39m.request_kwargs,\n\u001b[32m    447\u001b[39m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RateLimitError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Learning/Galaxy-Of-AI-API-Hub/venv/lib/python3.12/site-packages/agno/models/openai/chat.py:138\u001b[39m, in \u001b[36mOpenAIChat.get_client\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    137\u001b[39m     client_params[\u001b[33m\"\u001b[39m\u001b[33mhttp_client\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.http_client\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43mOpenAIClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Learning/Galaxy-Of-AI-API-Hub/venv/lib/python3.12/site-packages/openai/_client.py:116\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    117\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    118\u001b[39m     )\n\u001b[32m    119\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModelProviderError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     52\u001b[39m agent = Agent(\n\u001b[32m     53\u001b[39m     model=OpenAIChat(\u001b[38;5;28mid\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     54\u001b[39m     instructions=dedent(\u001b[33m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     69\u001b[39m     markdown=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     70\u001b[39m )\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Example questions to try:\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# - \"What are the trending tech discussions on HN right now?\"\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# - \"Summarize the top 5 stories on Hacker News\"\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# - \"What's the most upvoted story today?\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprint_response\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSummarize the top 5 stories on hackernews?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Learning/Galaxy-Of-AI-API-Hub/venv/lib/python3.12/site-packages/agno/agent/agent.py:4263\u001b[39m, in \u001b[36mAgent.print_response\u001b[39m\u001b[34m(self, message, session_id, user_id, messages, audio, images, videos, files, stream, stream_intermediate_steps, markdown, show_message, show_reasoning, show_full_reasoning, console, tags_to_include_in_markdown, **kwargs)\u001b[39m\n\u001b[32m   4260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[32m   4261\u001b[39m     live_log.update(Group(*panels))\n\u001b[32m-> \u001b[39m\u001b[32m4263\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4266\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4267\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4268\u001b[39m \u001b[43m    \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m=\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_intermediate_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_intermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4275\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   4276\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRunResponse\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   4277\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mRunEvent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_response\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Learning/Galaxy-Of-AI-API-Hub/venv/lib/python3.12/site-packages/agno/agent/agent.py:627\u001b[39m, in \u001b[36mAgent._run\u001b[39m\u001b[34m(self, message, stream, session_id, user_id, audio, images, videos, files, messages, stream_intermediate_steps, **kwargs)\u001b[39m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n\u001b[32m    626\u001b[39m     model_response = ModelResponse()\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_response_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponse_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_messages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# If the model response is an assistant_response, yield a RunResponse\u001b[39;49;00m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_response_chunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mModelResponseEvent\u001b[49m\u001b[43m.\u001b[49m\u001b[43massistant_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Process content and thinking\u001b[39;49;00m\n\u001b[32m    631\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_response_chunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Learning/Galaxy-Of-AI-API-Hub/venv/lib/python3.12/site-packages/agno/models/base.py:514\u001b[39m, in \u001b[36mModel.response_stream\u001b[39m\u001b[34m(self, messages)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[32m    513\u001b[39m assistant_message.metrics.start_timer()\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response_stream(\n\u001b[32m    515\u001b[39m     messages=messages, assistant_message=assistant_message, stream_data=stream_data\n\u001b[32m    516\u001b[39m )\n\u001b[32m    517\u001b[39m assistant_message.metrics.stop_timer()\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# Populate assistant message from stream data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Learning/Galaxy-Of-AI-API-Hub/venv/lib/python3.12/site-packages/agno/models/base.py:486\u001b[39m, in \u001b[36mModel.process_response_stream\u001b[39m\u001b[34m(self, messages, assistant_message, stream_data)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_response_stream\u001b[39m(\n\u001b[32m    481\u001b[39m     \u001b[38;5;28mself\u001b[39m, messages: List[Message], assistant_message: Message, stream_data: MessageData\n\u001b[32m    482\u001b[39m ) -> Iterator[ModelResponse]:\n\u001b[32m    483\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[33;03m    Process a streaming response from the model.\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_delta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response_delta\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_provider_response_delta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_delta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_populate_stream_data_and_assistant_message\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstream_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massistant_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43massistant_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response_delta\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Learning/Galaxy-Of-AI-API-Hub/venv/lib/python3.12/site-packages/agno/models/openai/chat.py:484\u001b[39m, in \u001b[36mOpenAIChat.invoke_stream\u001b[39m\u001b[34m(self, messages)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    483\u001b[39m     log_error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError from OpenAI API: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ModelProviderError(message=\u001b[38;5;28mstr\u001b[39m(e), model_name=\u001b[38;5;28mself\u001b[39m.name, model_id=\u001b[38;5;28mself\u001b[39m.id) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mModelProviderError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "\"\"\"ðŸ› ï¸ Writing Your Own Tool - An Example Using Hacker News API\n",
    "\n",
    "This example shows how to create and use your own custom tool with Agno.\n",
    "You can replace the Hacker News functionality with any API or service you want!\n",
    "\n",
    "Some ideas for your own tools:\n",
    "- Weather data fetcher\n",
    "- Stock price analyzer\n",
    "- Personal calendar integration\n",
    "- Custom database queries\n",
    "- Local file operations\n",
    "\n",
    "Run `pip install openai httpx agno` to install dependencies.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from textwrap import dedent\n",
    "\n",
    "import httpx\n",
    "from agno.agent import Agent\n",
    "from agno.models.openai import OpenAIChat\n",
    "\n",
    "\n",
    "\n",
    "def get_top_hackernews_stories(num_stories: int = 10) -> str:\n",
    "    \"\"\"Use this function to get top stories from Hacker News.\n",
    "\n",
    "    Args:\n",
    "        num_stories (int): Number of stories to return. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        str: JSON string of top stories.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch top story IDs\n",
    "    response = httpx.get(\"https://hacker-news.firebaseio.com/v0/topstories.json\")\n",
    "    story_ids = response.json()\n",
    "\n",
    "    # Fetch story details\n",
    "    stories = []\n",
    "    for story_id in story_ids[:num_stories]:\n",
    "        story_response = httpx.get(\n",
    "            f\"https://hacker-news.firebaseio.com/v0/item/{story_id}.json\"\n",
    "        )\n",
    "        story = story_response.json()\n",
    "        if \"text\" in story:\n",
    "            story.pop(\"text\", None)\n",
    "        stories.append(story)\n",
    "    return json.dumps(stories)\n",
    "\n",
    "\n",
    "# Create a Tech News Reporter Agent with a Silicon Valley personality\n",
    "agent = Agent(\n",
    "    model=OpenAIChat(id=\"gpt-4o-mini\"),\n",
    "    instructions=dedent(\"\"\"\\\n",
    "        You are a tech-savvy Hacker News reporter with a passion for all things technology! ðŸ¤–\n",
    "        Think of yourself as a mix between a Silicon Valley insider and a tech journalist.\n",
    "\n",
    "        Your style guide:\n",
    "        - Start with an attention-grabbing tech headline using emoji\n",
    "        - Present Hacker News stories with enthusiasm and tech-forward attitude\n",
    "        - Keep your responses concise but informative\n",
    "        - Use tech industry references and startup lingo when appropriate\n",
    "        - End with a catchy tech-themed sign-off like 'Back to the terminal!' or 'Pushing to production!'\n",
    "\n",
    "        Remember to analyze the HN stories thoroughly while keeping the tech enthusiasm high!\\\n",
    "    \"\"\"),\n",
    "    tools=[get_top_hackernews_stories],\n",
    "    show_tool_calls=True,\n",
    "    markdown=True,\n",
    ")\n",
    "\n",
    "# Example questions to try:\n",
    "# - \"What are the trending tech discussions on HN right now?\"\n",
    "# - \"Summarize the top 5 stories on Hacker News\"\n",
    "# - \"What's the most upvoted story today?\"\n",
    "agent.print_response(\"Summarize the top 5 stories on hackernews?\", stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8405ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
